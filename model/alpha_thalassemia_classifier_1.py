# -*- coding: utf-8 -*-
"""alpha-thalassemia-classifier-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OrygBl64KPxLh8ZnD6fZ5wwOzECt1G0T

# Import libraries and dataset
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot
import pandas as pd
import numpy as np
# %matplotlib inline
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

alphanorm = pd.read_csv('alphanorm.csv', index_col = False)

alphanorm.head()

alphanorm.info()

"""# Handle categorical values"""

alphanorm['rbc'] = alphanorm['rbc'].fillna(alphanorm.groupby('phenotype')['rbc'].transform('mean'))
alphanorm['mch'] = alphanorm['mch'].fillna(alphanorm.groupby('phenotype')['mch'].transform('mean'))

for col in alphanorm.columns:
    if alphanorm[col].dtype != object:
        Q1 = alphanorm[col].quantile(0.25)
        Q3 = alphanorm[col].quantile(0.75)
        IQR = Q3 - Q1
        S = 1.5*IQR
        LB = Q1 - S
        UB = Q3 + S
        print(UB)
        alphanorm.loc[alphanorm[col] > UB,col] = UB
        alphanorm.loc[alphanorm[col] < LB,col] = LB

alphanorm = alphanorm.astype({'sex' : 'category', 'phenotype' : 'category'})

alphanorm['phenotype'] = alphanorm['phenotype'] == 'alpha carrier'
alphanorm['phenotype'] = alphanorm['phenotype'].replace({True:1, False:0})

"""# Distinguish features & label"""

X = alphanorm.drop('phenotype', axis=1)
y = alphanorm['phenotype']

categorical_vars = list((X.select_dtypes(include=['category'])).columns)
categorical_vars

X.head()

"""# Perform encoding & Split the dataset"""

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

one_hot = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot, categorical_vars)],
                                remainder= 'passthrough')
X = pd.DataFrame(transformer.fit_transform(X))

from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)
for train_index, test_index in split.split(alphanorm, alphanorm["phenotype"]):
    strat_train = alphanorm.loc[train_index]
    strat_test = alphanorm.loc[test_index]

train_X = strat_train.drop('phenotype', axis=1)
train_y = strat_train['phenotype']
test_x = strat_test.drop('phenotype', axis=1)
test_y = strat_test['phenotype']

one_hot1 = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot1, categorical_vars)],
                                remainder= 'passthrough')
train_X = transformer.fit_transform(train_X)
train_X = pd.DataFrame(train_X)

one_hot2 = OneHotEncoder()
transformer = ColumnTransformer([('one_hot', one_hot2, categorical_vars)],
                                remainder= 'passthrough')
test_x = transformer.fit_transform(test_x)
test_x = pd.DataFrame(test_x)

train_X.head()

"""# Perform normalization & over sampling"""

from sklearn import preprocessing
train_X = preprocessing.normalize(train_X)
test_x = preprocessing.normalize(test_x)

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state = 2)
train_X, train_y = sm.fit_resample(train_X, train_y)

train_X

train_y.value_counts()

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

# rf = RandomForestClassifier(n_estimators=50, criterion='gini', max_features='sqrt')
rf = RandomForestClassifier(n_estimators=100, criterion='entropy', max_features='log2', random_state=0)

rf.fit(train_X, train_y)

y_pred = rf.predict(test_x)

y_pred

print("Accuracy:", accuracy_score(test_y, y_pred))
print("Confusion Matrix:\n", confusion_matrix(test_y, y_pred))
print("Classification Report:\n", classification_report(test_y, y_pred))

"""# Bagging Classifier"""

# We train a Bagging Classifier with Random Forest as the base model.

from sklearn.ensemble import BaggingClassifier

bagging = BaggingClassifier(estimator=RandomForestClassifier(criterion='gini', max_features='sqrt'), n_estimators=50, random_state=0)

bagging.fit(train_X, train_y)

y_pred = bagging.predict(test_x)

accuracy_score(test_y, y_pred)

"""# AdaBoost Classifier"""

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(n_estimators=100, random_state=0, learning_rate=1.0)

ada.fit(train_X, train_y)

y_pred = ada.predict(test_x)

accuracy_score(test_y, y_pred)

"""
# KNN Classifier"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 10, metric='minkowski', p=5)

knn.fit(train_X, train_y)

print("accuracy score: ", knn.score(test_x, test_y))
print("confusion matrix: \n", confusion_matrix(test_y, knn.predict(test_x)))
print("classification report: ", classification_report(test_y, knn.predict(test_x)))

"""# Use Cross Validation to find best hyperparameter for RF"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

k = KFold(n_splits=10)

# Implement cross validation for diabetes data set
for i, j in k.split(X):
    # Use .iloc to select rows based on indices
    X_train_cv, X_test_cv = X.iloc[i], X.iloc[j]
    y_train_cv, y_test_cv = y.iloc[i], y.iloc[j]
    rf = RandomForestClassifier(n_estimators=100, criterion='entropy', max_features='log2', random_state=0)
    rf.fit(X_train_cv, y_train_cv)

print("cross_val_score: ", cross_val_score(rf, train_X, train_y, cv = 10))
print("\n")
print("mean cross_val_score: ",np.mean(cross_val_score(rf, train_X, train_y, cv = 10)))

"""# Define accuracies of models"""

rf_accuracy = cross_val_score(estimator=rf, X=train_X, y=train_y, cv=10)
knn_accuracy = cross_val_score(estimator=knn, X=train_X, y=train_y, cv=10)
ada_accuracy = cross_val_score(estimator=ada, X=train_X, y=train_y, cv=10)
bagging_accuracy = cross_val_score(estimator=bagging, X=train_X, y=train_y, cv=10)

print("RF Accuracy: {:.2f}%".format(rf_accuracy.mean()*100))
print("Ada Boost Accuracy: {:.2f}%".format(ada_accuracy.mean()*100))
print("KNN Accuracy: {:.2f}%".format(knn_accuracy.mean()*100))
print("Bagging Accuracy: {:.2f}%".format(bagging_accuracy.mean()*100))

"""# Use GridSearchCV to find the best hyperparameters"""

from sklearn.model_selection import GridSearchCV

"""## For RF Model"""

param_grid = {
    'n_estimators': [10, 50, 100],
    'max_features': ['auto', 'sqrt', 'log2'],
    'criterion': ['gini', 'entropy']
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)
grid_search.fit(train_X, train_y)

print("Best Hyperparameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

"""## For KNN Model"""

param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski', 'euclidean']
}

grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)
grid_search.fit(train_X, train_y)

print("Best Hyperparameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

"""## For Bagging Model"""

param_grid = {
    'n_estimators': [10, 50, 100],
    'max_samples': [0.5, 0.75, 1.0]
}

grid_search = GridSearchCV(estimator=bagging, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)
grid_search.fit(train_X, train_y)

print("Best Hyperparameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

"""## For AdaBoost Model"""

param_grid = {
    'n_estimators': [10, 50, 100],
    'learning_rate': [0.1, 0.5, 1.0]
}

grid_search = GridSearchCV(estimator=ada, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)
grid_search.fit(train_X, train_y)

print("Best Hyperparameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

"""# Create pickle"""

import joblib
 # Save the model as a pickle in a file
joblib.dump(rf, 'thalassemia_model.pkl')